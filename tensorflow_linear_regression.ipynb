{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_linear regression ",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhoQS85encycRpE3GQX5lT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ncdxhuanghao/tensorstudy/blob/master/tensorflow_linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsXVwMg_T1_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from  __future__ import absolute_import,division,print_function \n",
        "\n",
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "rng= np.random \n",
        "#Parameters \n",
        "learing_rate=0.01\n",
        "training_steps=1000\n",
        "display_step=50 \n",
        "\n",
        "#Traing Data \n",
        "X = np.array([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
        "Y = np.array([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
        "\n",
        "#Weight andBias, initialized randomly. \n",
        "W= tf.Variable(rng.randn(), name=\"weight\")\n",
        "b= tf.Variable(rng.randn(), name=\"bias\")\n",
        "#Linear reqression(Wx+b) \n",
        "def linear_regression(x):\n",
        "    return W*x +b \n",
        "#Mean square error\n",
        "def mean_square(y_pred, y_true):\n",
        "    return tf.reduce_mean(tf.square(y_pred-y_true))\n",
        "#Stochastic Gradient Descent Optimizer \n",
        "optimizer = tf.optimizers.SGD(learing_rate) \n",
        "\n",
        "#Optimization process \n",
        "def run_optimization():\n",
        "    #Wrap computation inside a GradientTape for automatic differentiation\n",
        "    with tf.GradientTape() as g:\n",
        "        pred = linear_regression(X)\n",
        "        loss = mean_square(pred,Y)\n",
        "    #Compute gradients\n",
        "    gradients =g.gradient(loss,[W,b])\n",
        "\n",
        "    #Upate W and b following gradients \n",
        "    optimizer.apply_gradients(zip(gradients,[W,b]))\n",
        "\n",
        "#Run training for the given number of steps. \n",
        "for step in range(1, training_steps +1):\n",
        "    #Run the optimization to update W and b values . \n",
        "    run_optimization() \n",
        "\n",
        "    if step % display_step ==0:\n",
        "        pred= linear_regression(X)\n",
        "        loss= mean_square(pred,Y) \n",
        "        print(\"step:%i,loss:%f,W:%f,b:%f\" %(step, loss,W.numpy(), b.numpy()))\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "#Graphic display \n",
        "plt.plot(X,Y,'ro',label=\"Original data\") \n",
        "plt.plot(X,np.array(W*X+b),label=\"Fitted line\") \n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}